{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1:\n",
    "Create one array of actual values and another array of predicted values. Compare the two sets\n",
    "with the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score,recall_score,f1_score,confusion_matrix,accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "actuel_va=np.array([0,1,0,0,0,0,0,1,1,1,1,1,1])\n",
    "predict_value=np.array([0,1,0,0,1,0,1,1,1,1,0,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "precis_neg=precision_score(actuel_va,predict_value,pos_label=0)\n",
    "precis_posit=precision_score(actuel_va,predict_value,pos_label=1)\n",
    "\n",
    "recall_neg=recall_score(actuel_va,predict_value,pos_label=0)\n",
    "recall_posit=recall_score(actuel_va,predict_value,pos_label=1)\n",
    "\n",
    "Precision_score=precision_score(actuel_va,predict_value)\n",
    "Recall_score=recall_score(actuel_va,predict_value)\n",
    "\n",
    "F1_score=(2*(Precision_score*Recall_score))/(Precision_score+Recall_score)\n",
    "accuracy=accuracy_score(actuel_va,predict_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Prcision negative score : 0.8\n",
      "\n",
      " Prcision positve score : 0.75\n",
      "\n",
      " Recall negative score : 0.6666666666666666\n",
      "\n",
      " Recall Positive score : 0.8571428571428571\n",
      "\n",
      " Prcision  score : 0.75\n",
      "\n",
      " Recall score : 0.8571428571428571\n",
      "\n",
      " f1_score  : 0.7999999999999999\n",
      "\n",
      " accuracy score  : 0.7692307692307693\n"
     ]
    }
   ],
   "source": [
    "print(f\" Prcision negative score : {precis_neg}\")\n",
    "print('')\n",
    "print(f\" Prcision positve score : {precis_posit}\")\n",
    "print('')\n",
    "print(f\" Recall negative score : {recall_neg}\")\n",
    "print('')\n",
    "print(f\" Recall Positive score : {recall_posit}\")\n",
    "print('')\n",
    "print(f\" Prcision  score : {Precision_score}\")\n",
    "print('')\n",
    "print(f\" Recall score : {Recall_score}\")\n",
    "print('')\n",
    "print(f\" f1_score  : {F1_score}\")\n",
    "print('')\n",
    "print(f\" accuracy score  : {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_Score=f1_score(actuel_va,predict_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " f1_score  : 0.7999999999999999\n"
     ]
    }
   ],
   "source": [
    "print(f\" f1_score  : {f1_Score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4 2]\n",
      " [1 6]]\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix=confusion_matrix(actuel_va,predict_value)\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2:\n",
    "Find out the recall, precision, F1 score and confusion matrix with picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP,TN,FP,FN=42,32,8,18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Precision=TP/(TP+FP)\n",
    "Recall=TP/(TP+FN)\n",
    "Accuracy=(TP+TN)/(TP+TP+FN+FP)\n",
    "F1_Score=(2*(Precision*Recall))/(Precision+Recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Prcision  score : 0.84\n",
      "\n",
      " Recall score : 0.7\n",
      "\n",
      " Accurancy score : 0.6727272727272727\n",
      "\n",
      " F1 score : 0.7636363636363636\n"
     ]
    }
   ],
   "source": [
    "print(f\" Prcision  score : {Precision}\")\n",
    "print('')\n",
    "print(f\" Recall score : {Recall}\")\n",
    "print('')\n",
    "print(f\" Accurancy score : {Accuracy}\")\n",
    "print('')\n",
    "print(f\" F1 score : {F1_Score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
